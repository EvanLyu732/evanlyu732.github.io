<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>


    
        <link rel="alternate" type="application/rss+xml" title="RSS" href="https://evanlyu732.github.io/atom.xml">
    
    
    
        
    
    
    
    
    
    
        
    
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>evan lyu</title>
    <meta charset="utf-8">
    <meta name="title" content="evan lyu">
    <meta name="description" content="">
    <meta property="og:image:width" content="200" />
    <meta property="og:image:height" content="200" />
    <link rel="stylesheet" href="https:&#x2F;&#x2F;evanlyu732.github.io/style.css">
    
    
    <style>
    @media screen and (min-width: 320px) {
        body {
            font-size: calc(16px + 2 * ((100vw - 320px) / 960));
        }
    }
    </style>
    
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Source+Serif+Pro:wght@400;700&display=swap">
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:wght@400;700&display=swap">
    <style>body { font-family: 'Source Serif Pro', 'Source Han Serif SC', 'Noto Serif CJK SC', 'Noto Serif SC', serif }</style>
    
</head>
<body>
    
    <header class="header">
        <div class="blog-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io" class="logo">evan lyu</a></div>
        <nav class="navbar">
    <ul class="menu">
        
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;" class="current-menu-item-link">主页</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;archives" class="menu-item-link">归档</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;about" class="menu-item-link">关于</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;note" class="menu-item-link">书架</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;atom.xml" class="menu-item-link">订阅</a>
            
        </li>
        
    </ul>
</nav>

    </header>
    
    <main class="main">
        
<section class="posts">
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog37&#x2F;" class="post-title-link">计算机的未来?=神经网络框架</a></div>
        <div class="post-content">
            
                <p>今天看到了Geo hotz发了一篇<a href="https://x.com/realGeorgeHotz/status/1838062958410342437">推文</a>. 推文如下:</p>
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/hotz-twitter.png" height="100" width="100"/>
<p>如果不了解他的话, 这里补充一下背景tinygrad就是他一直在working的项目, 类似于pytorch这样的神经网络框架. 简单总结一下他的观点: 未来的计算机boot之后直接进入一个神经网络. 而神经网络依赖于神经网络框架. 所以tinygrad就像硬件层一样, 神经网络是软件层. 因此神经网络框架就等于操作系统. 其实就是<a href="https://evanlyu732.github.io/blog8/">LLM OS</a>的一种分解. 此外没有了CPU也就没有了内存问题, 神经网络只存在data flow而不存在control flow.</p>
<p>最后让claude点评一下hotz的推文.</p>
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/hotz-claud.png" height="100" width="100"/>
<p>第一次用LLM去点评其他人的观点. 还有有点被惊叹到的. 有理有据而且这个回答还是比较有说服力.</p>
<p>尽管我是hotz的粉丝, 上研究生的时候经常会去看他编程的直播. 但这篇推文目前时间点感觉还是两个字.</p>
<p>Too hype.</p>
<hr />
<p>footnote: hotz的想法完全有可能达到, 只要有无限context window和解决了幻觉的<a href="https://evanlyu732.github.io/blog12/">llm</a>.</p>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年九月廿三日

            
        </div>
    </article>
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog33&#x2F;" class="post-title-link">深度强化学习101</a></div>
        <div class="post-content">
            
                <p>记录一下接触深度强化学习的学习历程. 材料选自Pieter Abbeel的<a href="https://www.youtube.com/watch?v=2GwBez0D20A">Foundation of Deep RL Series.</a>. 整篇文章追求知识点的连惯性. 所以会补充详细的公式和历史背景. 这篇文章会一直持续更新. 也算是对之前说要写的llya30做一次原型测试吧. 由于会采用不用来源的资料所以概念会有重叠, 好处是可以从不同角度看到一个概念的描述. 方便更全面的理解.</p>
<h2 id="mu-lu">目录</h2>
<ul>
<li><a href="https://evanlyu732.github.io/blog33/#foundations-of-deep-reinforcement-learning-with-pieter-abbeel">Foundations of Deep Reinforcement Learning with Pieter Abbeel</a>
<ul>
<li><a href="https://evanlyu732.github.io/blog33/#lecture-1-mdps-exact-solution-methods-max-ent-rl">Lecture 1: MDPs, Exact Solution Methods, Max-ent RL</a></li>
<li><a href="https://evanlyu732.github.io/blog33/#lecture-2-deep-q-learning">Lecture 2: Deep Q-Learning</a></li>
<li><a href="https://evanlyu732.github.io/blog33/#lecture-3-policy-gradients-advantage-estimation">Lecture 3: Policy Gradients, Advantage Estimation</a></li>
<li><a href="https://evanlyu732.github.io/blog33/#lecture-4-trpo-ppo">Lecture 4: TRPO, PPO</a></li>
<li><a href="https://evanlyu732.github.io/blog33/#lecture-5-ddpg-sac">Lecture 5: DDPG, SAC</a></li>
<li><a href="https://evanlyu732.github.io/blog33/#lecture-6-model-based-rl">Lecture 6: Model-based RL</a></li>
</ul>
</li>
<li><a href="https://evanlyu732.github.io/blog33/#cs-285deep-reinforcement-learning">CS 285:Deep Reinforcement Learning</a></li>
<li><a href="https://evanlyu732.github.io/blog33/#ucl-course-on-rl">UCL Course on RL</a></li>
<li><a href="https://evanlyu732.github.io/blog33/#%E9%A1%B9%E7%9B%AE">项目</a></li>
<li><a href="https://evanlyu732.github.io/blog33/#where-to-go-next">Where to Go Next?</a></li>
</ul>
<p><a id="foundations-of-deep-reinforcement-learning-with-pieter-abbeel"></a></p>
<h1 id="foundations-of-deep-reinforcement-learning-with-pieter-abbeel">Foundations of Deep Reinforcement Learning with Pieter Abbeel</h1>
<p><a id="lecture-1-mdps-exact-solution-methods-max-ent-rl"></a></p>
<h2 id="lecture-1-mdps-exact-solution-methods-max-ent-rl">Lecture 1: MDPs, Exact Solution Methods, Max-ent RL</h2>
<p>点击<a href="https://www.dropbox.com/scl/fi/rvbpc40ozhstnwhk7h5b7/l1-mdps-exact-methods.pdf?rlkey=5bibe5t8cqmpm9dhth969iiq6&amp;e=1&amp;dl=0">这里</a>进入lecture1的slides. 一句话总结lecture1的内容: 强化学习可以表示为马尔可夫决策过程(Markov Decision Processes, MDPs). 解MDP有两种基本策略, Value Iteration和Policy Iteration. 另外, 引入Entropy带MDP可以提升MDP的鲁棒性.</p>
<ul>
<li>
<p>MDP: 需要了解什么是MDP. 先要了解什么是马尔可夫链(Markov chain). Markov为了证明独立同分布不是大数定律(Law of large numbers)满足的前提, 在1906年发表的文章里面提出了马尔可夫链(Markov chain). 如果需要进一步了解Markov chain, 推荐这个<a href="https://www.youtube.com/watch?v=CIe869Rce2k">视频</a></p>
<ul>
<li>Markov chain: 前提下一颗的状态 只取决于当前时刻 ,转移矩阵 </li>
<li>frist-order Markov chain &amp; n-order Markov chain:</li>
<li>Markov chains convergence: 我们在前面说到了Markov chain的提出是为了证明大数定律在非独立同分布的假设前提下, 依然成立. 那么Markov chain是如何收敛的呢?</li>
</ul>
</li>
</ul>
<p>$$
\frac{n!}{k!(n-k)!} = \binom{n}{k}
$$</p>
<ul>
<li>
<p>Bellman equation: </p>
</li>
<li>
<p>Value Iteration:</p>
</li>
<li>
<p>Policy Iteration:</p>
</li>
<li>
<p>Maximum Entropy Formulation:</p>
</li>
</ul>
<p>我们这里通过slide里面的grid world并结合代码来理解一下整个过程.</p>
<ul>
<li>小结:</li>
</ul>
<p><a id="lecture-2-deep-q-learning"></a></p>
<h2 id="lecture-2-deep-q-learning">Lecture 2: Deep Q-Learning</h2>
<p>点击<a href="https://www.dropbox.com/scl/fi/rvbpc40ozhstnwhk7h5b7/l1-mdps-exact-methods.pdf?rlkey=5bibe5t8cqmpm9dhth969iiq6&amp;e=1&amp;dl=0">这里</a>进入lecture2的slides. 一句话总结lecture2的内容: 由于实际情况下MDP的状态空间通常很大, 不适合直接用value iteration和policy iteration来解复杂的MDP. Q-Learning通过在样本空间进行采样来解复杂的MDP. 而Q-Learning也需要表格存储状态转移信息. DQN通过使用神经网络去采样来近似Q函数.</p>
<ul>
<li>
<p>Q-Learning:</p>
</li>
<li>
<p>DQN:</p>
</li>
</ul>
<p><a id="lecture-3-policy-gradients-advantage-estimation"></a></p>
<h2 id="lecture-3-policy-gradients-advantage-estimation">Lecture 3: Policy Gradients, Advantage Estimation</h2>
<p>点击<a href="https://www.dropbox.com/scl/fi/htn2r6ac807oluoxeihdt/l3-policy-gradient-and-advantage-estimation.pdf?rlkey=26hsbd5qvthb8ozq53vdfjrr4&amp;e=1&amp;dl=0">这里</a>进入lecture3的slides. 一句话总结lecture3的内容: 由于Value Iteration无法表示下一步的动作, 并且Q-learning在高维空间下没办法有效求解. 相比与Value Iteration和Q-learning, Policy Gradients的表示更加简单. 而Policy Gradients通过Likelihood Ratio Graident求解出来的是Path. 需要使用Temporal decomposition分解成State和Action. 需要使用Baseline subtraction去改变最终选择的Path. 在使用公式去分解的时候, 需要使用Value function estimation求解. 而在Advantage Estimation介绍一些更加进阶的Value function estimation方法.</p>
<ul>
<li>Policy Gradients:</li>
<li>Temporal Decomposition:</li>
<li>Baseline subtraction:</li>
<li>Value function estimation:</li>
<li>Advantage Estimation(A2C/A3C/GAE):</li>
</ul>
<p><a id="lecture-4-trpo-ppo"></a></p>
<h2 id="lecture-4-trpo-ppo">Lecture 4: TRPO, PPO</h2>
<p>点击<a href="https://www.dropbox.com/scl/fi/z0ev7f53yoyilrkfl9jck/l4-TRPO-PPO.pdf?rlkey=1y8f0am0bpqyxysxq3adnkobu&amp;e=1&amp;dl=0">这里</a>进入lecture4的slides. 一句话总结lecture4的内容: 在Lecture 3中, 我们知道Policy Gradients可以求解出特定的Path. 但是在求解特定的Path的过程中, 步长(step-size)的大小该如何确定呢? TRPO解决了这个问题. 而TRPO需要second order来求解, 而不太方便做优化. PPO作为TRPO的改进, 降阶为first-order优化问题.</p>
<ul>
<li>Surrogate loss: </li>
<li>Step-sizing and Trust Region Policy Optimization (TRPO):</li>
<li>Proximal Policy Optimization (PPO):</li>
</ul>
<p><a id="lecture-5-ddpg-sac"></a></p>
<h2 id="lecture-5-ddpg-sac">Lecture 5: DDPG, SAC</h2>
<p>点击<a href="https://www.dropbox.com/scl/fi/302ef41a9929yvtedc77l/l5-DDPG-SAC.pdf?rlkey=xc21zgliwfjynjse1je8oo6mx&amp;dl=0">这里</a>进入lecture5的slides. 一句话总结lecture5的内容: 对于特定场景, on-policy方法采样复杂度会比较高. 而DDPG解决了这个问题. SAC通过引入entropy保证了更好的收敛和防止过拟合.</p>
<ul>
<li>Deep Deterministic Policy Gradient (DDPG):</li>
<li>Soft Actor Critic (SAC):</li>
</ul>
<p><a id="lecture-6-model-based-rl"></a></p>
<h2 id="lecture-6-model-based-rl">Lecture 6: Model-based RL</h2>
<p>点击<a href="https://www.dropbox.com/scl/fi/pnv0k74lbajh2uahoegwr/l6-model-based-rl.pdf?rlkey=psemgw6b6a4c4owmc16liyba0&amp;dl=0">这里</a>进入lecture6的slides. 一句话总结lecture6的内容: 前面5个lecture讲的都是model-free RL. 所谓model-free RL是指agent直接通过action来迭代policy而不需要学习环境.  model-based RL通过对环境建模再进行policy迭代.</p>
<ul>
<li>Model-based RL:</li>
<li>Robust Model-based RL:Model-EnsembleTRPO (ME-TRPO):</li>
<li>Adaptive Model-based RL: Model-based Meta-Policy Optimization (MB-MPO):</li>
</ul>
<p><a id="cs-285deep-reinforcement-learning"></a></p>
<h1 id="cs-285-deep-reinforcement-learning">CS 285:Deep Reinforcement Learning</h1>
<p>这一部分采用CS 285来补充一下上一部分没提到的一些内容.</p>
<p><a id="ucl-course-on-rl"></a></p>
<h1 id="ucl-course-on-rl">UCL Course on RL</h1>
<p><a id="项目"></a></p>
<h1 id="xiang-mu">项目</h1>
<p><a id="where-to-go-next"></a></p>
<h1 id="where-to-go-next">Where to Go Next?</h1>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年九月三日

            
        </div>
    </article>
    
    <article class="post">
        <div class="post-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;blog32&#x2F;" class="post-title-link">智力复利</a></div>
        <div class="post-content">
            
                <p>最近在重新回顾Naval写的书, 在网页版的最后Naval推荐了一系列读物. 其中有一个<a href="https://x.com/zaoyang/status/940409514875961344">twitter thread</a>是Naval标记是Must Read. 之前一直没注意到.
看了一遍有点感触. 这篇文章是这篇twitter thread的原文集合. 原文在<a href="https://x.com/zaoyang/status/940409514875961344">这里</a>. (Twitter thread on “intellectual compounding” by @Zaoyang)</p>
<p>1/ There's a concept known as financial compounding, but most people don't know about intellectual compounding. Buffett and Munger employed this to great effect and to accumulate mental models such that they can make large decisions quickly. Intuition is simply reading a lot.</p>
<p>2/ This allows people to convert typically slow thinking and bad fast thinking (bad intuition) into good intuition. In academic literature this is known as Type 1 and Type 2 thinking. Most people don't accumulate enough of knowledge the tree.</p>
<p>3/ One of the common patterns for a self made billionaires is their ability to self study, self reason and accumulate a set of their own mental model. Intellectual capital compounds at a hidden rate and most people use tangible badges and net worth as measures.</p>
<p>4/ Intellectual capital is filling out the decision tree and is a forward looking view and net worth is backward looking. Just like how the wrong way to value tech companies with network effects is revenue but instead by their retention rate and network effects.</p>
<p>5/ Most people value themselves on their individual badges, which is what society labels for you. But most of what is predictive in life, is how you make decisions. Being effective and investing your time in the right area at the right time is a skill, not purely luck.</p>
<p>6/ Allowing readings to fill your mind takes advantage of your diffuse/focused mode of thinking and type 1 and type 2 decision making. Meaning you can focus and use your creative brain (diffuse mode) to wander and make associations. This is the key toward STEM education.</p>
<p>7/ More and more, just like the last 20 years was focused on physical athletes, the next 20 years would be focused on mental atheletes. These ways of thinking and compounding, Buffett, Munger, and polymaths have already used to a large degree and have been confirmed by academia.</p>
<p>8/ As the world get faster and faster with AR/VR, AI, crypto. The ability to invest in your own intellectual capital is a crucial prerequisite to maintaining and succeeding in this world and also for your children.</p>
<p>9/ While politicians say it's education that's important, it's only partially true, it's the ability to assimilate knowledge trees and compound knowledge that leads to satisfaction, mental stimulation, and long term wealth.</p>
<p>10/ What politicians are doing now is simply aiming backwards, but how can you scale this to everyone? It turns out that charter school have been doing a grand experiment.</p>
<p>11/ Taking children from disadvantaged backgrounds and making them fit for college. This has worked and three of the results are the following.</p>
<p>12/ &quot;Growth mindset&quot; not &quot;Fixed mindset&quot; &quot;Motivation mindset&quot; not &quot;Fixed mindset&quot; &quot;Student directed AND teacher directed education and projects&quot;</p>
<p>13/ In turns out statistically those three items are the most predictive. What are they? I'm glad you asked.</p>
<p>14/ It turns out that if you just tell students that their mind is like a &quot;muscle&quot; and spend just 10 minutes explaining that concept they will improve their grades dramatically.</p>
<p>15/ It turns out this concept is for a person's mind and for each skill set. People can have verbal &quot;fixed&quot; mindsets, humor &quot;fixed&quot; mindsets, math &quot;fixed&quot; mindsets. Almost everything. So, you have to consciously unlearn this and apply it consciously even if you know it.</p>
<p>16/ Why is this? It turns out people are criticized by society and labeled. So even if you have a &quot;growth&quot; mindset for physical items, you don't have it for mental items. This applies not only for students but also for adults. You always hear &quot;That's not me.&quot; It's a label.</p>
<p>17/ There's another concept called &quot;motivational mindset&quot; which teaches the person &quot;what good looks like&quot; which simply teaches the kid to follow &quot;go do the extra problems&quot; &quot;go to office hours&quot; &quot;do more problems&quot; Follow the process of the &quot;motivated student&quot; and it will work.</p>
<p>18/ It turns out these two concepts turn someone that's socio economically disadvantaged similar to the education status of someone who grew up upper middle class. This is not a panacea as a lot of people have so much stressors in their lives that they can't study.</p>
<p>19/ The last concept is student self directed project plus teacher lectures is the best. This surprised me, but conceptually, it gives the student agency and motivation. Most students have been so battered down by the system that they can't do this, but that's another story.</p>
<p>20/ These concepts have to be mind beliefs. Just like in Dune how they cite &quot;fear is the mindkiller&quot; These concepts have to be constantly applied to adults and children as they opposite tends to be pervasive and insidious.</p>
<p>21/ These are mindsets and as for strategies. People need to take concepts from Learning how to Learn by <a href="https://x.com/BarbaraOakley">@barbaraoakley</a> and <a href="https://x.com/sejnowski">@sejnowski</a>
and Art of Learning from Josh Waitzkin. They are are a manual for your brain.</p>
<p>22/ You thought just because you own a brain you knew how to operate it right? Why do you think the drop out rates for STEM is so high. Most people attribute it to pipeline or professor, but perhaps it's because people don't know how to learn difficult subjects.</p>
<p>23/ @LHTL_MOOC takes you from beginner to intermediate, and art of learning takes you from expert to being world class. Then you have Cal Newport's material, and those three resources are simply the best that I know of to hack your own brain</p>
<p>24/ Do you know anymore? Would love to know more resources and techniques.</p>
<p>25/ In conclusion, as the world become more and more technical and complex, most people don't have the mindset nor tactical skills. In short, people have to re-learn the manual to their own brain. Just because you have a computer, it doesn't mean you know everything about it.</p>

            
        </div>
        <div class="post-meta">
            
                


二〇二四年八月五日

            
        </div>
    </article>
    

</section>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

    <script>
        var Home = location.href,
        xhr,
        xhrUrl = '';
    
        var Diaspora = {
            L: function(url, f, err) {
                if (url == xhrUrl) {
                    return false;
                }
                xhrUrl = url;
                if (xhr) {
                    xhr.abort();
                }
                xhr = $.ajax({
                    type: 'GET',
                    url: url,
                    timeout: 10000,
                    success: function(data) {
                        f(data);
                        xhrUrl = '';
                    },
                    error: function(a, b, c) {
                        if (b == 'abort') {
                            err && err()
                        } else {
                            window.location.href = url;
                        }
                        xhrUrl = '';
                    }
                });
            },
            loading: function() {
                var w = window.innerWidth;
                var css = '<style class="loaderstyle" id="loaderstyle'+ w +'">'+
                    '@-moz-keyframes loader'+ w +'{100%{background-position:'+ w +'px 0}}'+
                    '@-webkit-keyframes loader'+ w +'{100%{background-position:'+ w +'px 0}}'+
                    '.loader'+ w +'{-webkit-animation:loader'+ w +' 3s linear infinite;-moz-animation:loader'+ w +' 3s linear infinite;}'+
                    '</style>';
                $('.loaderstyle').remove()
                $('head').append(css)
                $('#loader').removeClass().addClass('loader'+ w).show()
            },
            loaded: function() {
                $('#loader').removeClass().hide()
            }
        };
    
        $(function() {
            $('body').on('click', function(e) {
                var tag = $(e.target).attr('class') || '',
                    rel = $(e.target).attr('rel') || '';
                if (!tag && !rel) return;
                switch (true) {
                    // next page
                    case (tag.indexOf('more') != -1):
                        tag = $('.more');
                        if (tag.data('status') == 'loading') {
                            return false
                        }
                        var num = parseInt(tag.data('page')) || 1;
                        if (num == 1) {
                            tag.data('page', 1)
                        }
                        tag.html("旧文").data('status', 'loading')
                        Diaspora.loading()
                        Diaspora.L(tag.attr('href'), function(data) {
                            tag.hide();
                            $('.license').hide();
                            var link = $(data).find('.more').attr('href');
                            if (link) {
                                tag.attr('href', link).html("旧文").data('status', 'loaded')
                                tag.data('page', parseInt(tag.data('page')) + 1)
                            }

                            var tempScrollTop = $(window).scrollTop();
                            $('body').append($(data).find('.posts'))
                            $(window).scrollTop(tempScrollTop + 100);
                            Diaspora.loaded()
                            //$('html,body').animate({ scrollTop: tempScrollTop + 400 }, 500);
                            if (link !== '/' && link != '') {
                                $('body').append($(data).find('.page-nav'))
                            }
                        }, function() {
                            tag.html("旧文").data('status', 'loaded')
                        })
                        return false;
                        break;
                    default:
                        return true;
                        break;
                }
            });
        })
    </script>
    
    <nav class="page-nav">
      <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;page&#x2F;4&#x2F;" class="more">旧文</a>
    </nav>


    </main>
    
    <p class="license"></p>
    
</body>
</html>
