<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>


    
        <link rel="alternate" type="application/rss+xml" title="RSS" href="https://evanlyu732.github.io/atom.xml">
    
    
    
        
    
    
    
    
    
    
        
    
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>从零开始跟着Ilya Sutskever掌握90%的人工智能 - evan lyu</title>
    <meta charset="utf-8">
    <meta name="title" content="从零开始跟着Ilya Sutskever掌握90%的人工智能 - evan lyu">
    <meta name="description" content="">
    <meta property="og:image:width" content="200" />
    <meta property="og:image:height" content="200" />
    <link rel="stylesheet" href="https:&#x2F;&#x2F;evanlyu732.github.io/style.css">
    
    
    <style>
    @media screen and (min-width: 320px) {
        body {
            font-size: calc(16px + 2 * ((100vw - 320px) / 960));
        }
    }
    </style>
    
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Source+Serif+Pro:wght@400;700&display=swap">
    <link rel="stylesheet" href="https://fonts.loli.net/css2?family=Noto+Serif+SC:wght@400;700&display=swap">
    <style>body { font-family: 'Source Serif Pro', 'Source Han Serif SC', 'Noto Serif CJK SC', 'Noto Serif SC', serif }</style>
    
</head>
<body>
    
    <header class="header">
        <div class="blog-title"><a href="https:&#x2F;&#x2F;evanlyu732.github.io" class="logo">evan lyu</a></div>
        <nav class="navbar">
    <ul class="menu">
        
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;" class="menu-item-link">主页</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;archives" class="menu-item-link">归档</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;about" class="menu-item-link">关于</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;note" class="menu-item-link">书架</a>
            
        </li>
        
        <li class="menu-item">
            
            
            
            
            <a href="https:&#x2F;&#x2F;evanlyu732.github.io&#x2F;atom.xml" class="menu-item-link">订阅</a>
            
        </li>
        
    </ul>
</nav>

    </header>
    
    <main class="main">
        
<article class="post">
    <div class="post-title">
        <h1 class="title">从零开始跟着Ilya Sutskever掌握90%的人工智能</h1>
    </div>
    <div class="post-content">
        <p><a href="https://en.wikipedia.org/wiki/John_Carmack">John Carmack</a>让<a href="https://en.wikipedia.org/wiki/Ilya_Sutskever">Ilya Sutskever</a>推荐关于了解当下人工智能的阅读资料. Ilya Sutskever给了大约有40篇研究论文的链接.</p>
<blockquote>
<p>If you really learn all of these, you’ll know 90% of what matters today. -  Ilya Sutskever</p>
<p>如果你真的学会了这些，你就掌握了当今90%重要的内容 - Ilya Sutskever</p>
</blockquote>
<p>大概在一个月以前, <a href="https://x.com/keshavchan/status/1787861946173186062">推特</a>上流传一份<a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE">文件夹</a>包含30篇资料(论文, 书籍, 博客)包含了这些资料.</p>
<img src="https://raw.githubusercontent.com/EvanLyu732/evanlyu732.github.io/main/static/images/30u30.png" height="50" width="50"/>
<h1 id="yue-du-lie-biao">阅读列表</h1>
<p>按照发布的时间线从旧到新排列如下:</p>
<table><thead><tr><th><div style="width:290px">Time</div></th><th>Paper</th></tr></thead><tbody>
<tr><td>1993</td><td><a href="https://www.cs.toronto.edu/~hinton/absps/colt93.pdf">Keeping Neural Networks Simple by Minimizing the Description Length of the Weights</a></td></tr>
<tr><td>2004</td><td><a href="https://arxiv.org/pdf/math/0406077">A Tutorial Introduction to the Minimum Description Length Principle</a></td></tr>
<tr><td>2008</td><td><a href="https://www.vetta.org/documents/Machine_Super_Intelligence.pdf">Machine Super Intelligence</a></td></tr>
<tr><td>2011</td><td><a href="https://scottaaronson.blog/?p=762">The First Law of Complexodynamics</a></td></tr>
<tr><td>2012</td><td><a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a></td></tr>
<tr><td>2014</td><td><a href="https://arxiv.org/pdf/1405.6903">Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton</a></td></tr>
<tr><td>2014</td><td><a href="https://arxiv.org/pdf/1410.5401">Neural Turing Machines</a></td></tr>
<tr><td>2015</td><td><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></td></tr>
<tr><td>2015</td><td><a href="https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE">Understanding LSTM Networks</a></td></tr>
<tr><td>2015</td><td><a href="https://arxiv.org/pdf/1409.2329">Recurrent Neural Network Regularization</a></td></tr>
<tr><td>2015</td><td><a href="https://arxiv.org/pdf/1512.03385">Deep Residual Learning for Image Recognition</a></td></tr>
<tr><td>2015</td><td><a href="https://arxiv.org/pdf/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></td></tr>
<tr><td>2015</td><td><a href="https://arxiv.org/pdf/1512.02595">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</a></td></tr>
<tr><td>2016</td><td><a href="https://arxiv.org/pdf/1511.06391">Order Matters: Sequence to Sequence for Sets</a></td></tr>
<tr><td>2016</td><td><a href="https://arxiv.org/pdf/1511.07122">Multi-Scale Context Aggregation by Dilated Convolutions</a></td></tr>
<tr><td>2016</td><td><a href="https://arxiv.org/pdf/1603.05027">Identity Mappings in Deep Residual Networks</a></td></tr>
<tr><td>2017</td><td><a href="https://arxiv.org/pdf/1506.03134">Pointer Networks</a></td></tr>
<tr><td>2017</td><td><a href="https://arxiv.org/pdf/1704.01212">Neural Message Passing for Quantum Chemistry</a></td></tr>
<tr><td>2017</td><td><a href="https://arxiv.org/pdf/1706.03762">Attention Is All You Need</a></td></tr>
<tr><td>2017</td><td><a href="https://arxiv.org/pdf/1706.01427">A simple neural network module for relational reasoning</a></td></tr>
<tr><td>2017</td><td><a href="https://arxiv.org/pdf/1611.02731">Variational Lossy Autoencoder</a></td></tr>
<tr><td>2017</td><td><a href="https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf">Kolmogorov Complexity and Algorithmic Randomness</a></td></tr>
<tr><td>2018</td><td><a href="https://arxiv.org/pdf/1806.01822">Relational Recurrent Neural Networks</a></td></tr>
<tr><td>2019</td><td><a href="https://arxiv.org/pdf/1811.06965">GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</a></td></tr>
<tr><td>2020</td><td><a href="https://arxiv.org/pdf/2001.08361">Scaling Laws for Neural Language Models</a></td></tr>
<tr><td>2022</td><td><a href="https://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a></td></tr>
<tr><td>2024</td><td><a href="https://cs231n.github.io/">CS231n: Convolutional Neural Networks for Visual Recognition</a></td></tr>
</tbody></table>
<h1 id="ji-hua">计划</h1>
<p>写30篇文章深度解析Ilya推荐的每一份资料, 并深度实践每一篇资料的概念.</p>
<h1 id="wei-shi-yao-yao-zuo-zhe-ge-shi">为什么要做这个事</h1>
<ul>
<li>现在关于贩卖人工智能焦虑的文章越来越多. 真正有价值的文章都会有特定阅读群体. 这个系列定位是所有人. </li>
<li>互联网上的信息总是零散的多, 能够串联起来形成完成的线的文章少. </li>
<li>中文互联网需要更多有价值的内容, 我认为这个过程是有价值的. </li>
<li>对自己来说也是一个学习的过程, 检验自己是否真的理解. 任何经不起质疑的观点都是错的.</li>
</ul>
<h2 id="qian-ti">前提</h2>
<p>虽然说是从零, 但是不会包括编程的教学.</p>
<ul>
<li>需要能够基本使用Python.</li>
<li>有一定英文阅读能力</li>
</ul>
<h2 id="jie-guo">结果</h2>
<p>看完这30篇文章并实践之后, 你能够:</p>
<ul>
<li>了解每一个篇文章提出的背景, 定义, 原理, 实现, 应用.</li>
<li>能够通过代码实现每一个步骤, 并能论证作者的推理. 每一篇文章都会有对应的Jupter Notebook.</li>
<li>按照时间线理解30篇文章, 所以会知道发展的脉络. 对未来的发展方向会有更清晰的理解.</li>
<li>按照Ilya说的, 能够理解90%的人工智能.</li>
</ul>
<h1 id="xiang-guan-lian-jie">相关链接</h1>
<ul>
<li><a href="https://aman.ai/primers/ai/top-30-papers/">Ilya Sutskever's Top 30</a></li>
<li><a href="https://mfaulk.github.io/2024/06/19/ilya-papers.html">Ilya’s AI papers: Key Takeaways</a></li>
</ul>

    </div>
    <div class="post-meta">
        <span class="post-time">
        
            


二〇二四年六月廿六日

        
        </span>
    </div>
</article>

<div class="prev_next">
<nav id="prev_next">
    <div class="prev">
        
    </div>
    <div class="next">
        
    </div>
</nav>
</div>

<div class="post-comment">




</div>

    </main>
    
    <p class="license"></p>
    
</body>
</html>
